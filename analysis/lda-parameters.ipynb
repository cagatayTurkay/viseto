{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Models with Varying Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Apply LDA with different values for alpha and beta parameters. Export the models for visualisation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents_facebook(filename):\n",
    "    'Return a list of documents ready for topic modelling.'\n",
    "    with open(filename) as f:\n",
    "        posts = json.load(f)\n",
    "        return [post['message'] for post in posts if 'message' in post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_documents_facebook('../data/facebook.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(docs):\n",
    "    'Return a bag-of-word representation of the documents after cleaning (stopwords/punctuation removal, stemming).'\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation + '“”’—') \n",
    "    lemma = WordNetLemmatizer()\n",
    "    return [clean(doc, stop, exclude, lemma) for doc in docs]\n",
    "\n",
    "def clean(doc, stop, exclude, lemma):\n",
    "    punc_free_doc = ''.join(c for c in doc if c not in exclude)\n",
    "    stop_free_words = [w for w in punc_free_doc.lower().split() if w not in stop]    \n",
    "    normalized_words = [lemma.lemmatize(w) for w in stop_free_words if len(w) >= 3]\n",
    "    return normalized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Syrian military declared today that the U.S.-Russia brokered cease-fire is over, blaming rebel groups for violating the agreement. ['syrian', 'military', 'declared', 'today', 'usrussia', 'brokered', 'ceasefire', 'blaming', 'rebel', 'group', 'violating', 'agreement']\n",
      "\n",
      "Rose Pak, an influential community activist who turned San Francisco's Asian-American population into a political power in the city, passes away at 68. ['rose', 'pak', 'influential', 'community', 'activist', 'turned', 'san', 'franciscos', 'asianamerican', 'population', 'political', 'power', 'city', 'pass', 'away']\n",
      "\n",
      "Warplanes target the besieged Syrian city of Aleppo for the first time since the ceasefire went into effect last week. ['warplane', 'target', 'besieged', 'syrian', 'city', 'aleppo', 'first', 'time', 'since', 'ceasefire', 'went', 'effect', 'last', 'week']\n",
      "\n",
      "Using Skittles to make a point about “our Syrian refugee problem” didn’t go over too well with Mars, Incorporated—but the candy maker’s rebuke of Donald Trump Jr..’s controversial tweet is hardly a first. ['using', 'skittle', 'make', 'point', 'syrian', 'refugee', 'problem', 'didnt', 'well', 'mar', 'incorporatedbut', 'candy', 'maker', 'rebuke', 'donald', 'trump', 'jr', 'controversial', 'tweet', 'hardly', 'first']\n",
      "\n",
      "Rep. Elijah Cummings criticizes Mylan CEO Heather Bresch during a congressional hearing over the drastic rise in the cost of EpiPens: \"After Mylan takes our punches, they will fly back to their mansions in their private jets and laugh all the way to the bank while our constituents suffer, file for bankruptcy, and watch their children get sicker, and in some cases die.\" ['rep', 'elijah', 'cummings', 'criticizes', 'mylan', 'ceo', 'heather', 'bresch', 'congressional', 'hearing', 'drastic', 'rise', 'cost', 'epipens', 'mylan', 'take', 'punch', 'fly', 'back', 'mansion', 'private', 'jet', 'laugh', 'way', 'bank', 'constituent', 'suffer', 'file', 'bankruptcy', 'watch', 'child', 'get', 'sicker', 'case', 'die']\n",
      "\n",
      "BREAKING: Man believed to be person wanted in connection with NY and NJ bombings taken into custody, sources say. ['breaking', 'man', 'believed', 'person', 'wanted', 'connection', 'bombing', 'taken', 'custody', 'source', 'say']\n",
      "\n",
      "JUST IN: U.S. Federal Reserve maintains current interest rate level, but signals a rate hike is coming soon http://abcn.ws/2d9mPgr ['federal', 'reserve', 'maintains', 'current', 'interest', 'rate', 'level', 'signal', 'rate', 'hike', 'coming', 'soon', 'httpabcnws2d9mpgr']\n",
      "\n",
      "TONIGHT: Matthew Dowd and LZ Granderson host a very special edition of Strait Talk LIVE from Detroit. Have questions? Leave them below, and tune in tonight at 7:00 ET! http://abcn.ws/2d6hx9W ['tonight', 'matthew', 'dowd', 'granderson', 'host', 'special', 'edition', 'strait', 'talk', 'live', 'detroit', 'question', 'leave', 'tune', 'tonight', '700', 'httpabcnws2d6hx9w']\n",
      "\n",
      "Jesse Benton, Ron Paul's 2012 campaign chairman, was convicted of conspiracy, causing false campaign contribution reports to be filed to the Federal Election Commission and participating in a false statement scheme. ['jesse', 'benton', 'ron', 'paul', '2012', 'campaign', 'chairman', 'convicted', 'conspiracy', 'causing', 'false', 'campaign', 'contribution', 'report', 'filed', 'federal', 'election', 'commission', 'participating', 'false', 'statement', 'scheme']\n",
      "\n",
      "What's next for The Library of Congress? ABC News talks to new Librarian of Congress Carla Hayden to find out: abcn.ws/2cp0a38 ['whats', 'next', 'library', 'congress', 'abc', 'news', 'talk', 'new', 'librarian', 'congress', 'carla', 'hayden', 'find', 'abcnws2cp0a38']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = preprocess_documents(data)\n",
    "for i in range(0, 10):\n",
    "    print(data[i], docs[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 204),\n",
       " ('donald', 165),\n",
       " ('clinton', 110),\n",
       " ('hillary', 95),\n",
       " ('say', 80),\n",
       " ('president', 74),\n",
       " ('debate', 63),\n",
       " ('presidential', 51),\n",
       " ('obama', 47),\n",
       " ('first', 40)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = Counter(chain.from_iterable(docs))\n",
    "word_counts = sorted(word_dict.items(), key=lambda x: -x[1])\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find topics of the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lda(corpus, num_topics=10, passes=10, alpha='symmetric', eta=None):\n",
    "    'Return an LDA model from the given doc-term matrix .'\n",
    "    return LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes, alpha=alpha, eta=eta, random_state=0)\n",
    "\n",
    "def get_model_topics(lda):\n",
    "    return [[(lda.id2word[t], '{:.3f}'.format(p)) for t, p in lda.get_topic_terms(i)] for i in range(lda.num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('trump', '0.042'),\n",
       "  ('donald', '0.036'),\n",
       "  ('clinton', '0.025'),\n",
       "  ('hillary', '0.021'),\n",
       "  ('presidential', '0.014'),\n",
       "  ('debate', '0.013'),\n",
       "  ('republican', '0.008'),\n",
       "  ('campaign', '0.007'),\n",
       "  ('first', '0.007'),\n",
       "  ('white', '0.006')],\n",
       " [('say', '0.015'),\n",
       "  ('trump', '0.013'),\n",
       "  ('would', '0.012'),\n",
       "  ('donald', '0.011'),\n",
       "  ('republican', '0.007'),\n",
       "  ('voter', '0.007'),\n",
       "  ('excited', '0.006'),\n",
       "  ('think', '0.006'),\n",
       "  ('first', '0.006'),\n",
       "  ('house', '0.006')],\n",
       " [('president', '0.016'),\n",
       "  ('trump', '0.011'),\n",
       "  ('saying', '0.008'),\n",
       "  ('donald', '0.008'),\n",
       "  ('black', '0.008'),\n",
       "  ('obama', '0.007'),\n",
       "  ('american', '0.007'),\n",
       "  ('hillary', '0.007'),\n",
       "  ('clinton', '0.007'),\n",
       "  ('political', '0.006')],\n",
       " [('trump', '0.019'),\n",
       "  ('donald', '0.018'),\n",
       "  ('say', '0.014'),\n",
       "  ('new', '0.010'),\n",
       "  ('great', '0.008'),\n",
       "  ('president', '0.007'),\n",
       "  ('know', '0.007'),\n",
       "  ('campaign', '0.006'),\n",
       "  ('back', '0.006'),\n",
       "  ('debate', '0.006')],\n",
       " [('president', '0.016'),\n",
       "  ('obama', '0.016'),\n",
       "  ('year', '0.008'),\n",
       "  ('city', '0.008'),\n",
       "  ('trump', '0.007'),\n",
       "  ('donald', '0.007'),\n",
       "  ('world', '0.006'),\n",
       "  ('news', '0.006'),\n",
       "  ('abc', '0.006'),\n",
       "  ('tell', '0.005')],\n",
       " [('trump', '0.022'),\n",
       "  ('donald', '0.017'),\n",
       "  ('like', '0.013'),\n",
       "  ('say', '0.010'),\n",
       "  ('occupy', '0.010'),\n",
       "  ('page', '0.010'),\n",
       "  ('democrat', '0.010'),\n",
       "  ('people', '0.007'),\n",
       "  ('live', '0.007'),\n",
       "  ('black', '0.006')],\n",
       " [('new', '0.018'),\n",
       "  ('trump', '0.013'),\n",
       "  ('president', '0.013'),\n",
       "  ('york', '0.010'),\n",
       "  ('donald', '0.009'),\n",
       "  ('say', '0.008'),\n",
       "  ('jersey', '0.008'),\n",
       "  ('obama', '0.006'),\n",
       "  ('hillary', '0.006'),\n",
       "  ('clinton', '0.006')],\n",
       " [('trump', '0.013'),\n",
       "  ('donald', '0.009'),\n",
       "  ('presidential', '0.008'),\n",
       "  ('election', '0.008'),\n",
       "  ('debate', '0.007'),\n",
       "  ('people', '0.006'),\n",
       "  ('republican', '0.006'),\n",
       "  ('say', '0.005'),\n",
       "  ('president', '0.005'),\n",
       "  ('john', '0.005')],\n",
       " [('trump', '0.021'),\n",
       "  ('donald', '0.014'),\n",
       "  ('say', '0.014'),\n",
       "  ('clinton', '0.014'),\n",
       "  ('debate', '0.012'),\n",
       "  ('campaign', '0.012'),\n",
       "  ('president', '0.009'),\n",
       "  ('people', '0.007'),\n",
       "  ('election', '0.007'),\n",
       "  ('hillary', '0.007')],\n",
       " [('clinton', '0.029'),\n",
       "  ('trump', '0.028'),\n",
       "  ('hillary', '0.028'),\n",
       "  ('donald', '0.023'),\n",
       "  ('debate', '0.013'),\n",
       "  ('say', '0.012'),\n",
       "  ('president', '0.011'),\n",
       "  ('presidential', '0.011'),\n",
       "  ('candidate', '0.008'),\n",
       "  ('first', '0.007')]]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]    \n",
    "lda = build_lda(corpus, num_topics=10)\n",
    "get_model_topics(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get topics associated with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_for_documents(lda, corpus):\n",
    "    'Return topic probabilities for each document in the given corpus. Only topics with probability higher than 0.01.'\n",
    "    return [get_topics_for_one_document(lda, doc) for doc in corpus]\n",
    "\n",
    "def get_topics_for_one_document(lda, doc):\n",
    "    return [float('{:.3f}'.format(p)) for (t, p) in lda.get_document_topics(doc, minimum_probability=0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.931],\n",
       " [0.944],\n",
       " [0.94],\n",
       " [0.959],\n",
       " [0.975],\n",
       " [0.925],\n",
       " [0.936],\n",
       " [0.95],\n",
       " [0.961],\n",
       " [0.94]]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_topics = get_topics_for_documents(lda, corpus)\n",
    "output_topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find terms associated with topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms_for_topics(lda):\n",
    "    'Return term probabilities for each topic. Only top 10 terms.'\n",
    "    return [get_terms_for_one_topic(lda, i) for i in range(lda.num_topics)]\n",
    "\n",
    "def get_terms_for_one_topic(lda, topic_id):\n",
    "    return [float('{:.3f}'.format(p)) for t, p in lda.get_topic_terms(topic_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.042, 0.036, 0.025, 0.021, 0.014, 0.013, 0.008, 0.007, 0.007, 0.006],\n",
       " [0.015, 0.013, 0.012, 0.011, 0.007, 0.007, 0.006, 0.006, 0.006, 0.006],\n",
       " [0.016, 0.011, 0.008, 0.008, 0.008, 0.007, 0.007, 0.007, 0.007, 0.006],\n",
       " [0.019, 0.018, 0.014, 0.01, 0.008, 0.007, 0.007, 0.006, 0.006, 0.006],\n",
       " [0.016, 0.016, 0.008, 0.008, 0.007, 0.007, 0.006, 0.006, 0.006, 0.005],\n",
       " [0.022, 0.017, 0.013, 0.01, 0.01, 0.01, 0.01, 0.007, 0.007, 0.006],\n",
       " [0.018, 0.013, 0.013, 0.01, 0.009, 0.008, 0.008, 0.006, 0.006, 0.006],\n",
       " [0.013, 0.009, 0.008, 0.008, 0.007, 0.006, 0.006, 0.005, 0.005, 0.005],\n",
       " [0.021, 0.014, 0.014, 0.014, 0.012, 0.012, 0.009, 0.007, 0.007, 0.007],\n",
       " [0.029, 0.028, 0.028, 0.023, 0.013, 0.012, 0.011, 0.011, 0.008, 0.007]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_terms = get_terms_for_topics(lda)\n",
    "output_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_data(lda, corpus):\n",
    "    'Return a dictionary detailing model parameters and probability matrices.'\n",
    "    return {\n",
    "        'alpha': float(lda.alpha[0]), # assume the same value for all elements\n",
    "        'beta': float(lda.eta[0]), # assume the same value for all elements\n",
    "        'num_topics': lda.num_topics,\n",
    "        'doc_topics': get_topics_for_documents(lda, corpus),\n",
    "        'topic_terms': get_terms_for_topics(lda)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_for_alphas(corpus, alphas, filename):\n",
    "    data = [export_model_data(build_lda(corpus, alpha=alpha), corpus) for alpha in alphas]\n",
    "    save_file(data, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data_for_alphas(corpus, [0.01, 0.03, 0.1, 0.3, 1, 3, 10], '../data/facebook-alphas.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
